{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3995fd1",
   "metadata": {
    "papermill": {
     "duration": 0.011527,
     "end_time": "2023-07-09T16:32:52.546116",
     "exception": false,
     "start_time": "2023-07-09T16:32:52.534589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Attempting to do machine translation following the [original seq2seq paper](https://paperswithcode.com/method/seq2seq). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb0441",
   "metadata": {
    "papermill": {
     "duration": 0.008949,
     "end_time": "2023-07-09T16:32:52.565040",
     "exception": false,
     "start_time": "2023-07-09T16:32:52.556091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I will solve this problem in two parts.\n",
    "\n",
    " Part 1  : Converting the sentences into sequences. This will include removing NaN values, basic pre-processing (removing punctuation, converting to lower-case), tokenization and vocabulary creation.\n",
    "\n",
    "Part 2 : Building and training the seq2seq model, following the [paper](https://paperswithcode.com/method/seq2seq) closely(relation between input-output of encoder-decoder,number of layers in LSTM etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad82f7e",
   "metadata": {
    "papermill": {
     "duration": 0.008671,
     "end_time": "2023-07-09T16:32:52.582939",
     "exception": false,
     "start_time": "2023-07-09T16:32:52.574268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Part 1 : Converting to sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f101d",
   "metadata": {
    "papermill": {
     "duration": 0.008701,
     "end_time": "2023-07-09T16:32:52.600618",
     "exception": false,
     "start_time": "2023-07-09T16:32:52.591917",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Removing Nan Values, Converting to lower case and removing punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b8a6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:32:52.621351Z",
     "iopub.status.busy": "2023-07-09T16:32:52.620835Z",
     "iopub.status.idle": "2023-07-09T16:33:01.590590Z",
     "shell.execute_reply": "2023-07-09T16:33:01.589446Z"
    },
    "papermill": {
     "duration": 8.984061,
     "end_time": "2023-07-09T16:33:01.593798",
     "exception": false,
     "start_time": "2023-07-09T16:32:52.609737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ted</td>\n",
       "      <td>politicians do not have permission to do what ...</td>\n",
       "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ted</td>\n",
       "      <td>id like to tell you about one such child</td>\n",
       "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>this percentage is even greater than the perce...</td>\n",
       "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ted</td>\n",
       "      <td>what we really mean is that theyre bad at not ...</td>\n",
       "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>the ending portion of these vedas is called up...</td>\n",
       "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127602</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>examples of art deco construction can be found...</td>\n",
       "      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127603</th>\n",
       "      <td>ted</td>\n",
       "      <td>and put it in our cheeks</td>\n",
       "      <td>और अपने गालों में डाल लेते हैं।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127604</th>\n",
       "      <td>tides</td>\n",
       "      <td>as for the other derivatives of sulphur  the c...</td>\n",
       "      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है  दे...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127605</th>\n",
       "      <td>tides</td>\n",
       "      <td>its complicated functioning is defined thus in...</td>\n",
       "      <td>Zरचनाप्रकिया को उसने एक पहेली में यों बांधा है</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127606</th>\n",
       "      <td>ted</td>\n",
       "      <td>theyve just won four government contracts to b...</td>\n",
       "      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127605 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                   english_sentence  \\\n",
       "0             ted  politicians do not have permission to do what ...   \n",
       "1             ted           id like to tell you about one such child   \n",
       "2       indic2012  this percentage is even greater than the perce...   \n",
       "3             ted  what we really mean is that theyre bad at not ...   \n",
       "4       indic2012  the ending portion of these vedas is called up...   \n",
       "...           ...                                                ...   \n",
       "127602  indic2012  examples of art deco construction can be found...   \n",
       "127603        ted                           and put it in our cheeks   \n",
       "127604      tides  as for the other derivatives of sulphur  the c...   \n",
       "127605      tides  its complicated functioning is defined thus in...   \n",
       "127606        ted  theyve just won four government contracts to b...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "0       राजनीतिज्ञों के पास जो कार्य करना चाहिए वह करन...  \n",
       "1       मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी  \n",
       "2        यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
       "3          हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
       "4             इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n",
       "...                                                   ...  \n",
       "127602  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...  \n",
       "127603                    और अपने गालों में डाल लेते हैं।  \n",
       "127604  जहां तक गंधक के अन्य उत्पादों का प्रश्न है  दे...  \n",
       "127605    Zरचनाप्रकिया को उसने एक पहेली में यों बांधा है   \n",
       "127606  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n",
       "\n",
       "[127605 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/english-hindi-machine-translation/Hindi_English_Truncated_Corpus.csv')\n",
    "\n",
    "df = df.dropna()  # Remove NaN values\n",
    "\n",
    "# Converting English sentences to lowercase and removing punctuations from both languages\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "df['english_sentence'] = df['english_sentence'].str.lower().apply(remove_punctuation)\n",
    "df['hindi_sentence'] = df['hindi_sentence'].apply(remove_punctuation)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ed450",
   "metadata": {
    "papermill": {
     "duration": 0.009165,
     "end_time": "2023-07-09T16:33:01.612558",
     "exception": false,
     "start_time": "2023-07-09T16:33:01.603393",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before tokenizing or creating vocabularies split the data into train, validation and test. This prevents \"information leakage\" into the test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ac7f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:01.634188Z",
     "iopub.status.busy": "2023-07-09T16:33:01.633079Z",
     "iopub.status.idle": "2023-07-09T16:33:01.687623Z",
     "shell.execute_reply": "2023-07-09T16:33:01.686501Z"
    },
    "papermill": {
     "duration": 0.068293,
     "end_time": "2023-07-09T16:33:01.690504",
     "exception": false,
     "start_time": "2023-07-09T16:33:01.622211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452e2747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:01.712273Z",
     "iopub.status.busy": "2023-07-09T16:33:01.711868Z",
     "iopub.status.idle": "2023-07-09T16:33:01.870668Z",
     "shell.execute_reply": "2023-07-09T16:33:01.869356Z"
    },
    "papermill": {
     "duration": 0.172354,
     "end_time": "2023-07-09T16:33:01.873275",
     "exception": false,
     "start_time": "2023-07-09T16:33:01.700921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(source              102084\n",
       " english_sentence    102084\n",
       " hindi_sentence      102084\n",
       " dtype: int64,\n",
       " source              12760\n",
       " english_sentence    12760\n",
       " hindi_sentence      12760\n",
       " dtype: int64,\n",
       " source              12761\n",
       " english_sentence    12761\n",
       " hindi_sentence      12761\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count(),val_df.count(),test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd6db7",
   "metadata": {
    "papermill": {
     "duration": 0.009279,
     "end_time": "2023-07-09T16:33:01.892491",
     "exception": false,
     "start_time": "2023-07-09T16:33:01.883212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tokenizing the sentences. Source sentences are tokenized in reverse as it was one of the key source of improvement in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb65b2d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:01.914363Z",
     "iopub.status.busy": "2023-07-09T16:33:01.913927Z",
     "iopub.status.idle": "2023-07-09T16:33:03.307197Z",
     "shell.execute_reply": "2023-07-09T16:33:03.305965Z"
    },
    "papermill": {
     "duration": 1.407717,
     "end_time": "2023-07-09T16:33:03.309866",
     "exception": false,
     "start_time": "2023-07-09T16:33:01.902149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82661</th>\n",
       "      <td>tides</td>\n",
       "      <td>[SOS, unions, trade, strong, of, up, building,...</td>\n",
       "      <td>[SOS, इसलिए, मजदूर, वर्ग, की, पहली, जरूरत, यह,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121426</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>[SOS, 1830, of, decade, the, during, marble, i...</td>\n",
       "      <td>[SOS, इस, तथ्य, के, भी, कोई, साक्ष्य, नहीं, है...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30572</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>[SOS, pradesh, uttar, in, districts, 70, are, ...</td>\n",
       "      <td>[SOS, उत्तर, प्रदेश, में, ७०, जिले, हैं, EOS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25371</th>\n",
       "      <td>ted</td>\n",
       "      <td>[SOS, schoolhouse, the, to, way, the, on, scho...</td>\n",
       "      <td>[SOS, या, तो, स्कूल, में, या, स्कूल, आतेजाते, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56266</th>\n",
       "      <td>indic2012</td>\n",
       "      <td>[SOS, road, northsouth, long, the, crosses, it...</td>\n",
       "      <td>[SOS, इसके, बाद, एक, बडा़, खुला, स्थान, है, जह...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19147</th>\n",
       "      <td>tides</td>\n",
       "      <td>[SOS, control, to, difficult, more, seem, pku,...</td>\n",
       "      <td>[SOS, फ्खू, जैस, उत्परिवर्तन, पर, नियंत्रण, पा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26135</th>\n",
       "      <td>ted</td>\n",
       "      <td>[SOS, dollars, 5000, around, costing, was, tim...</td>\n",
       "      <td>[SOS, उस, समय, 5000, डालर, की, थी, EOS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77079</th>\n",
       "      <td>tides</td>\n",
       "      <td>[SOS, them, harmonise, to, attempt, an, make, ...</td>\n",
       "      <td>[SOS, दार्शनिक, और, इतिहास, उनमें, से, एक, या,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39591</th>\n",
       "      <td>tides</td>\n",
       "      <td>[SOS, 4344, pages, see, order, on, goods, of, ...</td>\n",
       "      <td>[SOS, डिपाजऋटि, के, बारे, में, जऋऊण्श्छ्ष्यादा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14882</th>\n",
       "      <td>tides</td>\n",
       "      <td>[SOS, it, of, amount, some, needs, badly, subc...</td>\n",
       "      <td>[SOS, यह, सच, है, कि, वाजपेयी, ने, एक, पक्के, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                   english_sentence  \\\n",
       "82661       tides  [SOS, unions, trade, strong, of, up, building,...   \n",
       "121426  indic2012  [SOS, 1830, of, decade, the, during, marble, i...   \n",
       "30572   indic2012  [SOS, pradesh, uttar, in, districts, 70, are, ...   \n",
       "25371         ted  [SOS, schoolhouse, the, to, way, the, on, scho...   \n",
       "56266   indic2012  [SOS, road, northsouth, long, the, crosses, it...   \n",
       "19147       tides  [SOS, control, to, difficult, more, seem, pku,...   \n",
       "26135         ted  [SOS, dollars, 5000, around, costing, was, tim...   \n",
       "77079       tides  [SOS, them, harmonise, to, attempt, an, make, ...   \n",
       "39591       tides  [SOS, 4344, pages, see, order, on, goods, of, ...   \n",
       "14882       tides  [SOS, it, of, amount, some, needs, badly, subc...   \n",
       "\n",
       "                                           hindi_sentence  \n",
       "82661   [SOS, इसलिए, मजदूर, वर्ग, की, पहली, जरूरत, यह,...  \n",
       "121426  [SOS, इस, तथ्य, के, भी, कोई, साक्ष्य, नहीं, है...  \n",
       "30572       [SOS, उत्तर, प्रदेश, में, ७०, जिले, हैं, EOS]  \n",
       "25371   [SOS, या, तो, स्कूल, में, या, स्कूल, आतेजाते, ...  \n",
       "56266   [SOS, इसके, बाद, एक, बडा़, खुला, स्थान, है, जह...  \n",
       "19147   [SOS, फ्खू, जैस, उत्परिवर्तन, पर, नियंत्रण, पा...  \n",
       "26135             [SOS, उस, समय, 5000, डालर, की, थी, EOS]  \n",
       "77079   [SOS, दार्शनिक, और, इतिहास, उनमें, से, एक, या,...  \n",
       "39591   [SOS, डिपाजऋटि, के, बारे, में, जऋऊण्श्छ्ष्यादा...  \n",
       "14882   [SOS, यह, सच, है, कि, वाजपेयी, ने, एक, पक्के, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tokens\n",
    "START_TOKEN = 'SOS'\n",
    "END_TOKEN = 'EOS'\n",
    "OUT_OF_VOCAB_TOKEN = 'OOV'\n",
    "\n",
    "\n",
    "# Tokenize the sentences and add EOS and SOS tokens\n",
    "train_df['english_sentence'] = train_df['english_sentence'].apply(lambda x: [START_TOKEN] + x.split()[::-1] + [END_TOKEN])\n",
    "train_df['hindi_sentence'] = train_df['hindi_sentence'].apply(lambda x: [START_TOKEN] + x.split() + [END_TOKEN])\n",
    "\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e889f7b",
   "metadata": {
    "papermill": {
     "duration": 0.009721,
     "end_time": "2023-07-09T16:33:03.329719",
     "exception": false,
     "start_time": "2023-07-09T16:33:03.319998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating the frequency counter , words having frequency =1 will not be included in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb81ff84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:03.351733Z",
     "iopub.status.busy": "2023-07-09T16:33:03.351301Z",
     "iopub.status.idle": "2023-07-09T16:33:04.379373Z",
     "shell.execute_reply": "2023-07-09T16:33:04.378172Z"
    },
    "papermill": {
     "duration": 1.042088,
     "end_time": "2023-07-09T16:33:04.381980",
     "exception": false,
     "start_time": "2023-07-09T16:33:03.339892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('the', 103848),\n",
       "  ('SOS', 102084),\n",
       "  ('EOS', 102084),\n",
       "  ('of', 59641),\n",
       "  ('and', 47380),\n",
       "  ('to', 38179),\n",
       "  ('in', 37816),\n",
       "  ('a', 29128),\n",
       "  ('is', 23864),\n",
       "  ('that', 14843)],\n",
       " [('SOS', 102084),\n",
       "  ('EOS', 102084),\n",
       "  ('के', 70335),\n",
       "  ('में', 51408),\n",
       "  ('है', 45851),\n",
       "  ('की', 39470),\n",
       "  ('और', 38008),\n",
       "  ('से', 30831),\n",
       "  ('का', 26611),\n",
       "  ('को', 25161)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define minimum word frequency for it to be included in vocabulary\n",
    "MIN_WORD_FREQ = 2\n",
    "\n",
    "# Count the frequency of each word in both languages\n",
    "english_vocab_counter = Counter(word for sentence in train_df['english_sentence'] for word in sentence)\n",
    "hindi_vocab_counter = Counter(word for sentence in train_df['hindi_sentence'] for word in sentence)\n",
    "\n",
    "english_vocab_counter.most_common(10) ,hindi_vocab_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aaf61d",
   "metadata": {
    "papermill": {
     "duration": 0.009916,
     "end_time": "2023-07-09T16:33:04.402084",
     "exception": false,
     "start_time": "2023-07-09T16:33:04.392168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating the vocabulary and adding the 'OOV' token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759beb5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:04.425045Z",
     "iopub.status.busy": "2023-07-09T16:33:04.424648Z",
     "iopub.status.idle": "2023-07-09T16:33:04.486016Z",
     "shell.execute_reply": "2023-07-09T16:33:04.484886Z"
    },
    "papermill": {
     "duration": 0.076526,
     "end_time": "2023-07-09T16:33:04.488880",
     "exception": false,
     "start_time": "2023-07-09T16:33:04.412354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create vocabulary by including words that have a frequency of more than MIN_WORD_FREQ\n",
    "english_vocab = {word: i for i, (word, freq) in enumerate(english_vocab_counter.items()) if freq >= MIN_WORD_FREQ}\n",
    "hindi_vocab = {word: i for i, (word, freq) in enumerate(hindi_vocab_counter.items()) if freq >= MIN_WORD_FREQ}\n",
    "# OOV token will be displayed when we encounter a word not in the vocabulary\n",
    "english_vocab.update({OUT_OF_VOCAB_TOKEN: len(english_vocab)})\n",
    "hindi_vocab.update({OUT_OF_VOCAB_TOKEN: len(hindi_vocab)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ba8ea",
   "metadata": {
    "papermill": {
     "duration": 0.009807,
     "end_time": "2023-07-09T16:33:04.509184",
     "exception": false,
     "start_time": "2023-07-09T16:33:04.499377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Finally, converting sentences to sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f06d2e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:04.533174Z",
     "iopub.status.busy": "2023-07-09T16:33:04.532738Z",
     "iopub.status.idle": "2023-07-09T16:33:06.486680Z",
     "shell.execute_reply": "2023-07-09T16:33:06.485498Z"
    },
    "papermill": {
     "duration": 1.969531,
     "end_time": "2023-07-09T16:33:06.489539",
     "exception": false,
     "start_time": "2023-07-09T16:33:04.520008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82661     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 41227, ...\n",
       "121426    [0, 28, 29, 30, 31, 32, 33, 34, 16, 9, 35, 36,...\n",
       "30572                       [0, 55, 56, 24, 57, 58, 16, 27]\n",
       "25371                   [0, 59, 60, 61, 24, 59, 61, 62, 27]\n",
       "56266     [0, 63, 64, 10, 65, 66, 67, 8, 68, 7, 69, 70, ...\n",
       "19147            [0, 74, 75, 76, 77, 78, 79, 80, 81, 8, 27]\n",
       "26135                        [0, 82, 83, 84, 85, 4, 86, 27]\n",
       "77079     [0, 87, 88, 89, 90, 91, 10, 59, 92, 30, 93, 94...\n",
       "39591     [0, 41227, 30, 104, 24, 105, 106, 30, 107, 108...\n",
       "14882     [0, 7, 119, 8, 9, 120, 41, 10, 121, 122, 4, 12...\n",
       "44539     [0, 131, 30, 132, 4, 133, 134, 24, 135, 41227,...\n",
       "77431                           [0, 169, 170, 171, 172, 27]\n",
       "84191                       [0, 173, 174, 34, 175, 176, 27]\n",
       "71777                                [0, 177, 178, 179, 27]\n",
       "92433     [0, 180, 181, 127, 182, 183, 8, 184, 185, 186,...\n",
       "77639     [0, 193, 194, 64, 195, 196, 197, 41, 198, 77, ...\n",
       "28978                                          [0, 205, 27]\n",
       "36665     [0, 206, 207, 208, 209, 45, 210, 77, 211, 212,...\n",
       "98741                   [0, 19, 223, 224, 225, 226, 16, 27]\n",
       "99709             [0, 227, 228, 229, 230, 142, 231, 73, 27]\n",
       "Name: hindi_sentence, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the words in the sentences to their corresponding index in the vocabulary\n",
    "train_df['english_sentence'] = train_df['english_sentence'].apply(lambda sentence: [english_vocab.get(word, english_vocab[OUT_OF_VOCAB_TOKEN]) for word in sentence])\n",
    "train_df['hindi_sentence'] = train_df['hindi_sentence'].apply(lambda sentence: [hindi_vocab.get(word, hindi_vocab[OUT_OF_VOCAB_TOKEN]) for word in sentence])\n",
    "train_df['hindi_sentence'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a6d6df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:06.512615Z",
     "iopub.status.busy": "2023-07-09T16:33:06.512201Z",
     "iopub.status.idle": "2023-07-09T16:33:06.537423Z",
     "shell.execute_reply": "2023-07-09T16:33:06.536259Z"
    },
    "papermill": {
     "duration": 0.039751,
     "end_time": "2023-07-09T16:33:06.539964",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.500213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82661     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 41227, ...\n",
       " 121426    [0, 28, 29, 30, 31, 32, 33, 34, 16, 9, 35, 36,...\n",
       " 30572                       [0, 55, 56, 24, 57, 58, 16, 27]\n",
       " 25371                   [0, 59, 60, 61, 24, 59, 61, 62, 27]\n",
       " 56266     [0, 63, 64, 10, 65, 66, 67, 8, 68, 7, 69, 70, ...\n",
       " 19147            [0, 74, 75, 76, 77, 78, 79, 80, 81, 8, 27]\n",
       " 26135                        [0, 82, 83, 84, 85, 4, 86, 27]\n",
       " 77079     [0, 87, 88, 89, 90, 91, 10, 59, 92, 30, 93, 94...\n",
       " 39591     [0, 41227, 30, 104, 24, 105, 106, 30, 107, 108...\n",
       " 14882     [0, 7, 119, 8, 9, 120, 41, 10, 121, 122, 4, 12...\n",
       " 44539     [0, 131, 30, 132, 4, 133, 134, 24, 135, 41227,...\n",
       " 77431                           [0, 169, 170, 171, 172, 27]\n",
       " 84191                       [0, 173, 174, 34, 175, 176, 27]\n",
       " 71777                                [0, 177, 178, 179, 27]\n",
       " 92433     [0, 180, 181, 127, 182, 183, 8, 184, 185, 186,...\n",
       " 77639     [0, 193, 194, 64, 195, 196, 197, 41, 198, 77, ...\n",
       " 28978                                          [0, 205, 27]\n",
       " 36665     [0, 206, 207, 208, 209, 45, 210, 77, 211, 212,...\n",
       " 98741                   [0, 19, 223, 224, 225, 226, 16, 27]\n",
       " 99709             [0, 227, 228, 229, 230, 142, 231, 73, 27]\n",
       " Name: hindi_sentence, dtype: object,\n",
       " 82661     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       " 121426    [0, 24, 4, 25, 7, 26, 27, 28, 29, 30, 31, 32, ...\n",
       " 30572                    [0, 49, 50, 8, 51, 52, 53, 48, 23]\n",
       " 25371          [0, 54, 7, 30, 55, 7, 56, 54, 7, 57, 58, 23]\n",
       " 56266     [0, 59, 60, 61, 7, 62, 63, 64, 65, 66, 67, 68,...\n",
       " 19147     [0, 71, 30, 72, 73, 74, 75, 76, 77, 78, 4, 79,...\n",
       " 26135               [0, 81, 82, 83, 84, 85, 86, 44, 57, 23]\n",
       " 77079     [0, 87, 32445, 30, 89, 90, 91, 92, 78, 7, 92, ...\n",
       " 39591     [0, 32445, 100, 101, 102, 56, 103, 4, 104, 105...\n",
       " 14882     [0, 63, 4, 109, 95, 110, 111, 112, 69, 17, 113...\n",
       " 44539     [0, 121, 122, 123, 124, 125, 17, 126, 127, 7, ...\n",
       " 77431                         [0, 44, 156, 157, 158, 7, 23]\n",
       " 84191                       [0, 32445, 160, 10, 161, 7, 23]\n",
       " 71777                                [0, 162, 163, 164, 23]\n",
       " 92433     [0, 41, 8, 165, 7, 92, 166, 28, 30, 167, 4, 16...\n",
       " 77639     [0, 174, 56, 175, 176, 90, 135, 177, 178, 17, ...\n",
       " 28978                                          [0, 184, 23]\n",
       " 36665     [0, 185, 186, 7, 30, 187, 188, 20, 145, 189, 1...\n",
       " 98741                            [0, 87, 198, 199, 200, 23]\n",
       " 99709     [0, 201, 202, 17, 203, 32445, 205, 115, 206, 2...\n",
       " Name: english_sentence, dtype: object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['hindi_sentence'][:20],train_df['english_sentence'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0ffec",
   "metadata": {
    "papermill": {
     "duration": 0.010369,
     "end_time": "2023-07-09T16:33:06.561041",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.550672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ecfceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:06.585483Z",
     "iopub.status.busy": "2023-07-09T16:33:06.584610Z",
     "iopub.status.idle": "2023-07-09T16:33:06.665849Z",
     "shell.execute_reply": "2023-07-09T16:33:06.664562Z"
    },
    "papermill": {
     "duration": 0.096491,
     "end_time": "2023-07-09T16:33:06.668840",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.572349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "?pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d775ff2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:06.693773Z",
     "iopub.status.busy": "2023-07-09T16:33:06.693362Z",
     "iopub.status.idle": "2023-07-09T16:33:06.705570Z",
     "shell.execute_reply": "2023-07-09T16:33:06.704447Z"
    },
    "papermill": {
     "duration": 0.027938,
     "end_time": "2023-07-09T16:33:06.707968",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.680030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    english_sequences, hindi_sequences = zip(*batch)\n",
    "    english_sequences = [torch.tensor(seq) for seq in english_sequences]\n",
    "    hindi_sequences = [torch.tensor(seq) for seq in hindi_sequences]\n",
    "    \n",
    "    # Pad sequences\n",
    "    english_sequences = pad_sequence(english_sequences, batch_first=True, padding_value=23)\n",
    "    hindi_sequences = pad_sequence(hindi_sequences, batch_first=True, padding_value=27)\n",
    "    \n",
    "    return english_sequences, hindi_sequences\n",
    "\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.df.iloc[idx]['english_sentence']), torch.tensor(self.df.iloc[idx]['hindi_sentence'])\n",
    "\n",
    "# Define a function to create data loaders\n",
    "def create_data_loaders(train_df, val_df, test_df, batch_size=4):\n",
    "    train_loader = DataLoader(TranslationDataset(train_df), batch_size=batch_size,collate_fn=collate_fn, shuffle=True)\n",
    "    val_loader = DataLoader(TranslationDataset(val_df),  collate_fn=collate_fn, batch_size=batch_size)\n",
    "    test_loader = DataLoader(TranslationDataset(test_df),  collate_fn=collate_fn, batch_size=batch_size)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_df, val_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94582cef",
   "metadata": {
    "papermill": {
     "duration": 0.010313,
     "end_time": "2023-07-09T16:33:06.728970",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.718657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Visualizing the data in the dataloaders\n",
    "\n",
    "Reduce the batch_size for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3be0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:06.752719Z",
     "iopub.status.busy": "2023-07-09T16:33:06.751849Z",
     "iopub.status.idle": "2023-07-09T16:33:06.860430Z",
     "shell.execute_reply": "2023-07-09T16:33:06.859553Z"
    },
    "papermill": {
     "duration": 0.124433,
     "end_time": "2023-07-09T16:33:06.864038",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.739605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4\n",
      "tensor([  0, 355, 299,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,\n",
      "         23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23])\n",
      "tensor([   0, 3184,  423,   27,   27,   27,   27,   27,   27,   27,   27,   27,\n",
      "          27,   27,   27,   27,   27,   27,   27,   27,   27,   27,   27,   27,\n",
      "          27,   27,   27,   27,   27,   27,   27,   27])\n",
      "tensor([    0,  7156,  2552,   520,     7,   449,  4807,    17, 32445,     7,\n",
      "            8,  6025,   222,     4,   848,   284,    68,   449,  7023,    85,\n",
      "         2159,     7,     4,  3012,  3424,  7824,     7,    23])\n",
      "tensor([    0,   795,    45,  1574,  3875,  1929,    45,    31,   277, 39376,\n",
      "           24, 32022,  1335,    88,    64,    30,  6287,    24,  6456,  3412,\n",
      "         1335,  3460,     4,  1390,   949,    30,  2767,  3460,   204,   287,\n",
      "          387,    27])\n",
      "tensor([   0, 2387, 3619,   23,   23,   23,   23,   23,   23,   23,   23,   23,\n",
      "          23,   23,   23,   23,   23,   23,   23,   23,   23,   23,   23,   23,\n",
      "          23,   23,   23,   23])\n",
      "tensor([   0, 2268, 2693,   27,   27,   27,   27,   27,   27,   27,   27,   27,\n",
      "          27,   27,   27,   27,   27,   27,   27,   27,   27,   27,   27,   27,\n",
      "          27,   27,   27,   27,   27,   27,   27,   27])\n",
      "tensor([   0, 1706,  220,   93,  719,  328, 1033,   30,   76, 3823,   23,   23,\n",
      "          23,   23,   23,   23,   23,   23,   23,   23,   23,   23,   23,   23,\n",
      "          23,   23,   23,   23])\n",
      "tensor([    0,  3595,  1314,   127,   128,    10,  1904,    30,   104,    24,\n",
      "         8755, 17616,    27,    27,    27,    27,    27,    27,    27,    27,\n",
      "           27,    27,    27,    27,    27,    27,    27,    27,    27,    27,\n",
      "           27,    27])\n",
      "torch.Size([4, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21/2115760384.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  english_sequences = [torch.tensor(seq) for seq in english_sequences]\n",
      "/tmp/ipykernel_21/2115760384.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hindi_sequences = [torch.tensor(seq) for seq in hindi_sequences]\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = next(dataiter)\n",
    "print(len(data[0]),len(data[1])) # = batch_size\n",
    "src , trg = data\n",
    "for i in range(len(src)):\n",
    "        print(src[i])\n",
    "        print(trg[i])\n",
    "print(src.shape) # (batch_size,length_of_sequences)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad68a0",
   "metadata": {
    "papermill": {
     "duration": 0.010534,
     "end_time": "2023-07-09T16:33:06.885625",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.875091",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This marks the end of the data preparation now we will define the seq2seq model and train it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70deec38",
   "metadata": {
    "papermill": {
     "duration": 0.010301,
     "end_time": "2023-07-09T16:33:06.906817",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.896516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First we will define the encoder, then the decoder and then combine them to define the seq2seq model. After this we will train the model and do validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967862e9",
   "metadata": {
    "papermill": {
     "duration": 0.010881,
     "end_time": "2023-07-09T16:33:06.928379",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.917498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Encoder \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2270f",
   "metadata": {
    "papermill": {
     "duration": 0.010452,
     "end_time": "2023-07-09T16:33:06.949726",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.939274",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Visualizing the embeddings. \n",
    "Again Reducing the embedding_dim for decent visualization. Can also use embed_example.**weight.data** to see the weight matrix of embeddings. \n",
    "\n",
    "Every word has a vector of size **embedding_dim** associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3efab0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:06.974554Z",
     "iopub.status.busy": "2023-07-09T16:33:06.973221Z",
     "iopub.status.idle": "2023-07-09T16:33:07.020886Z",
     "shell.execute_reply": "2023-07-09T16:33:07.019856Z"
    },
    "papermill": {
     "duration": 0.062243,
     "end_time": "2023-07-09T16:33:07.023213",
     "exception": false,
     "start_time": "2023-07-09T16:33:06.960970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5633, -0.7613,  0.6382,  0.6315, -1.3828, -0.0220, -1.2445,  0.5709,\n",
       "         -0.3984,  0.7237,  0.3416, -0.7214, -0.7264,  0.8674,  0.0475, -0.1087],\n",
       "        [-0.1380,  0.1000, -0.9086,  1.1975,  1.4482, -0.3140, -0.5042,  0.6986,\n",
       "          0.8681, -0.6629, -1.2156,  0.6783,  0.0185, -0.7259,  0.7487,  2.1836],\n",
       "        [-0.0807, -0.2238,  0.1027, -1.8286,  0.7095,  0.6904,  2.5964,  0.8055,\n",
       "         -1.1085,  0.2683,  1.5151, -1.1142,  1.4237, -0.4702, -1.1880, -0.3092],\n",
       "        [ 1.5518,  1.4537,  0.9522,  0.8927,  2.0210, -0.2655,  1.5467, -0.5011,\n",
       "          0.2822,  0.2574, -1.3491,  0.9236,  3.0456,  0.7430, -0.6057, -0.7287]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "input_dim = len(english_vocab)\n",
    "embedding_dim=16\n",
    "embed_example = nn.Embedding(input_dim, embedding_dim)\n",
    "embed_example\n",
    "embeddings = embed_example.weight.data\n",
    "embeddings[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee098fcb",
   "metadata": {
    "papermill": {
     "duration": 0.010746,
     "end_time": "2023-07-09T16:33:07.044968",
     "exception": false,
     "start_time": "2023-07-09T16:33:07.034222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Visualizing how dropout functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd5bdf2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:07.070289Z",
     "iopub.status.busy": "2023-07-09T16:33:07.069534Z",
     "iopub.status.idle": "2023-07-09T16:33:07.112716Z",
     "shell.execute_reply": "2023-07-09T16:33:07.111836Z"
    },
    "papermill": {
     "duration": 0.059167,
     "end_time": "2023-07-09T16:33:07.115535",
     "exception": false,
     "start_time": "2023-07-09T16:33:07.056368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1142, -0.1283, -0.9150,  ...,  0.4571, -0.4290, -0.5260],\n",
      "         [ 2.5914,  0.4068,  0.6622,  ...,  0.1918,  0.3976,  1.4082],\n",
      "         [-1.0043,  1.4600, -1.5398,  ..., -0.1862, -0.3014,  2.0512],\n",
      "         ...,\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278]],\n",
      "\n",
      "        [[ 0.1142, -0.1283, -0.9150,  ...,  0.4571, -0.4290, -0.5260],\n",
      "         [ 1.8804,  0.7643, -0.7959,  ...,  0.7558,  0.4821,  0.4082],\n",
      "         [ 0.4552, -0.9760, -1.6125,  ..., -0.7051,  0.9978,  0.9181],\n",
      "         ...,\n",
      "         [-0.6554, -0.8692,  1.5923,  ..., -0.3927, -0.7336, -2.0615],\n",
      "         [ 1.1552,  0.5541,  0.3543,  ..., -0.5692,  0.2688,  0.8878],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278]],\n",
      "\n",
      "        [[ 0.1142, -0.1283, -0.9150,  ...,  0.4571, -0.4290, -0.5260],\n",
      "         [-0.3253, -0.3183, -1.1601,  ..., -0.0989,  0.9289,  0.1391],\n",
      "         [ 0.3456, -1.3017,  0.4159,  ...,  0.7125, -1.0856,  1.0699],\n",
      "         ...,\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278]],\n",
      "\n",
      "        [[ 0.1142, -0.1283, -0.9150,  ...,  0.4571, -0.4290, -0.5260],\n",
      "         [-0.8568, -1.3407,  1.2535,  ..., -0.3237,  0.5723,  0.8627],\n",
      "         [-0.0291, -1.5025,  0.5017,  ..., -0.8569, -0.3890,  1.8071],\n",
      "         ...,\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278],\n",
      "         [-0.6021, -1.4109,  0.5248,  ...,  0.5734,  0.0155, -0.2278]]],\n",
      "       grad_fn=<EmbeddingBackward0>) torch.Size([4, 28, 16])\n",
      "tensor([[[ 0.0000,  0.8046, -1.1180,  ...,  0.3219, -1.2370,  0.0000],\n",
      "         [-0.0000,  0.1886,  0.0000,  ..., -0.4609,  0.1868, -0.4727],\n",
      "         [ 0.0000, -0.5714, -0.0000,  ..., -1.2122,  2.4022,  0.9170],\n",
      "         ...,\n",
      "         [-3.2063, -0.0000, -0.1443,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.1443,  ..., -1.0053,  0.9861, -2.4924],\n",
      "         [-0.0000, -4.0312, -0.1443,  ..., -1.0053,  0.0000, -0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.8046, -1.1180,  ...,  0.3219, -1.2370,  0.0000],\n",
      "         [ 2.3353,  0.0000, -0.0000,  ...,  1.7734,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.4891,  0.2922,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0000,  0.0000,  0.0000,  ..., -0.6385,  1.5163,  0.0000],\n",
      "         [ 0.0000, -2.2656, -4.1623,  ..., -0.0000,  2.6115, -1.0307],\n",
      "         [-3.2063, -0.0000, -0.1443,  ..., -0.0000,  0.0000, -2.4924]],\n",
      "\n",
      "        [[ 0.0000,  0.0000, -0.0000,  ...,  0.0000, -1.2370,  1.6359],\n",
      "         [-2.2925,  1.5137, -0.0000,  ...,  0.0000, -0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., -0.1503, -0.0000,  0.0741],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., -1.0053,  0.9861, -0.0000],\n",
      "         [-3.2063, -4.0312, -0.1443,  ..., -1.0053,  0.0000, -0.0000],\n",
      "         [-3.2063, -4.0312, -0.0000,  ..., -0.0000,  0.9861, -2.4924]],\n",
      "\n",
      "        [[ 1.0118,  0.0000, -1.1180,  ...,  0.3219, -1.2370,  0.0000],\n",
      "         [-1.6879, -0.0000, -0.8974,  ..., -0.8890,  0.0000, -0.0000],\n",
      "         [-4.6264, -0.0000, -1.6818,  ...,  0.1436,  0.0000, -0.6252],\n",
      "         ...,\n",
      "         [-3.2063, -4.0312, -0.0000,  ..., -1.0053,  0.0000, -2.4924],\n",
      "         [-3.2063, -0.0000, -0.0000,  ..., -1.0053,  0.0000, -0.0000],\n",
      "         [-0.0000, -0.0000, -0.0000,  ..., -1.0053,  0.9861, -0.0000]]],\n",
      "       grad_fn=<MulBackward0>) torch.Size([4, 28, 16])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "embedded=embedding(src)\n",
    "print(embedded,embedded.shape)\n",
    "dropout = nn.Dropout(0.5)\n",
    "embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "embedded=dropout(embedding(src))\n",
    "print(embedded,embedded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e87fd",
   "metadata": {
    "papermill": {
     "duration": 0.011005,
     "end_time": "2023-07-09T16:33:07.137873",
     "exception": false,
     "start_time": "2023-07-09T16:33:07.126868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Visualizing the LSTM with num_layers = 3.** \n",
    "* (hidden , cell) will have the values from all the layers stacked one over another. \n",
    "* The shape of hidden and cell will be (num_layers,batch_size,hidden_dim)\n",
    "* The input shape required by LSTM if batch_first=True is (batch_size,sequence_length,input_length).\n",
    "* In our case the batch_size = **32** , sequence_length is the **length of integer sequences**, input_length is = **embedding_dim**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4be35461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:07.162709Z",
     "iopub.status.busy": "2023-07-09T16:33:07.162253Z",
     "iopub.status.idle": "2023-07-09T16:33:07.258470Z",
     "shell.execute_reply": "2023-07-09T16:33:07.257499Z"
    },
    "papermill": {
     "duration": 0.113089,
     "end_time": "2023-07-09T16:33:07.262176",
     "exception": false,
     "start_time": "2023-07-09T16:33:07.149087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1721, -0.1985,  0.1973, -0.2764, -0.1204, -0.2277, -0.0452,\n",
      "          -0.1104,  0.1064, -0.1619, -0.2747,  0.0829, -0.1066, -0.2344,\n",
      "           0.1671,  0.1116,  0.2070,  0.0980,  0.3262,  0.1509],\n",
      "         [ 0.0846, -0.2682,  0.0512, -0.2109,  0.0399, -0.2178, -0.0642,\n",
      "           0.1358, -0.3584,  0.0268, -0.1120, -0.0938,  0.3835, -0.2323,\n",
      "          -0.0599,  0.0755,  0.1129,  0.2590,  0.3183,  0.2175],\n",
      "         [ 0.0921, -0.2663,  0.1675, -0.2652, -0.0566, -0.4089, -0.0888,\n",
      "           0.0254, -0.0530,  0.0987,  0.1629, -0.2235, -0.0694, -0.2750,\n",
      "           0.0862,  0.2463,  0.2717,  0.3436,  0.5060,  0.0202],\n",
      "         [ 0.1912,  0.0278,  0.2036, -0.3391, -0.0596, -0.1821,  0.0443,\n",
      "          -0.0048,  0.0123, -0.0503, -0.1012,  0.3097, -0.1153, -0.1413,\n",
      "           0.0286,  0.1502,  0.0647,  0.1248,  0.3103,  0.0258]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([1, 4, 20])\n",
      "tensor([[[ 0.2078, -0.4637,  0.3857, -0.5982, -0.3008, -0.3525, -0.0558,\n",
      "          -0.2931,  0.1906, -0.2322, -0.5045,  0.2355, -0.2972, -0.3767,\n",
      "           0.3519,  0.2599,  0.4125,  0.2500,  0.6359,  0.2732],\n",
      "         [ 0.1295, -0.6064,  0.1120, -0.4643,  0.0940, -0.6395, -0.4801,\n",
      "           0.2040, -0.5550,  0.0344, -0.3590, -0.2248,  0.5212, -0.5069,\n",
      "          -0.3506,  0.1850,  0.2764,  0.5220,  0.4105,  0.3032],\n",
      "         [ 0.1104, -0.7823,  0.3181, -0.6485, -0.1506, -0.6053, -0.2091,\n",
      "           0.0716, -0.0975,  0.1264,  0.3146, -0.4204, -0.1000, -0.4912,\n",
      "           0.4229,  0.5861,  0.5494,  0.7632,  0.7811,  0.0249],\n",
      "         [ 0.3243,  0.0521,  0.5601, -0.8021, -0.1700, -0.5677,  0.1016,\n",
      "          -0.0098,  0.0279, -0.1004, -0.1719,  0.7428, -0.2249, -0.2362,\n",
      "           0.0498,  0.2586,  0.1309,  0.2129,  0.6694,  0.0568]]],\n",
      "       grad_fn=<StackBackward0>) torch.Size([1, 4, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(16, 20, num_layers = 1, dropout = 0.5,batch_first=True)\n",
    "outputs, (hidden, cell) = lstm(embedded)\n",
    "print(hidden,hidden.shape)\n",
    "print(cell,cell.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34836300",
   "metadata": {
    "papermill": {
     "duration": 0.011109,
     "end_time": "2023-07-09T16:33:07.285031",
     "exception": false,
     "start_time": "2023-07-09T16:33:07.273922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Formally defining the Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19292d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-09T16:33:07.310467Z",
     "iopub.status.busy": "2023-07-09T16:33:07.309417Z",
     "iopub.status.idle": "2023-07-09T16:33:08.347876Z",
     "shell.execute_reply": "2023-07-09T16:33:08.346646Z"
    },
    "papermill": {
     "duration": 1.054279,
     "end_time": "2023-07-09T16:33:08.350926",
     "exception": false,
     "start_time": "2023-07-09T16:33:07.296647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        # For every word(or an integer in the input sequence) it creates a vector of size embedding_dim \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim) \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout = dropout,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        #Shape of src is (batch_size,length of padded sequence)\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #Shape of embedded is (batch_size,length of one padded sequence,embedding_dim)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # Shape of hidden and cell both is (n_layers, batch_size, hidden_dim)\n",
    "        return hidden, cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.234815,
   "end_time": "2023-07-09T16:33:10.390754",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-09T16:32:38.155939",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
